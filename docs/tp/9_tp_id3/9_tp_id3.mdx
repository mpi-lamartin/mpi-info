---
hide_table_of_contents: true
hide_title: false
cor: false
title: "TP : Arbres de décision ID3"
---

import useBaseUrl from '@docusaurus/useBaseUrl';
import ThemedImage from '@theme/ThemedImage';

## Introduction

Dans ce TP, nous allons implémenter l'algorithme ID3 pour classifier les survivants du Titanic. On rappelle que c'est un algorithme de classification supervisée : à partir de données d'entraînement connues, il construit un arbre de décision permettant de prédire la classe d'une nouvelle donnée.


## Partie 1 : Découverte des données

1. Créer un nouveau Codespace sur https://github.com/mpi-lamartin/titanic. Lire le README.md pour comprendre la structure du code. Compiler et exécuter le code fourni en exemple qui classifie les femmes comme survivantes.

2. Créer un fichier `src/id3.ml` dans lequel vous allez écrire le code du TP. Commencer par ajouter `open Csv_loader` en haut du fichier pour pouvoir utiliser le type `passenger` et les fonctions de chargement.

Écrire une fonction `stats` qui affiche le nombre de survivants et de décédés dans les données d'entraînement. On rappelle qu'on peut afficher une variable `x` avec : `Printf.printf "valeur de x : %d\n" x`. 

```ocaml
val stats : passenger list -> unit
(* Exemple d'affichage : "Survivants: 342, Décédés: 549" *)
```

Pour compiler et exécuter votre code, vous pouvez modifier le Makefile en remplaçant `woman_survive.ml` par `id3.ml`, puis utiliser dans le terminal :

```bash
make run
```

## Partie 2 : Attributs catégoriels

Pour construire un arbre de décision, nous devons transformer les attributs numériques en attributs catégoriels. Par exemple, l'âge sera transformé en catégories : "child" ($< 18$ ans), "adult" ($18-60$ ans), "senior" ($> 60$ ans).

On définit un type `attribute` des attributs que l'on utilisera pour construire l'arbre de décision :

```ocaml
type attribute =
  | Sex        (* "male" ou "female" *)
  | Pclass     (* "1", "2" ou "3" *)
  | AgeGroup   (* "child", "adult", "senior", "unknown" *)
  | FamilySize (* "alone", "small", "large" *)
  | Embarked   (* "S", "C", "Q", "unknown" *)
  | FareGroup  (* "low", "medium", "high", "unknown" *)
```

3. Écrire une fonction `attribute_values` qui retourne la liste des valeurs possibles pour un attribut :

```ocaml
val attribute_values : attribute -> string list
(* Exemple : attribute_values Sex = ["male"; "female"] *)
```

4. Écrire une fonction `get_attribute_value` qui retourne la valeur d'un attribut pour un passager :

```ocaml
val get_attribute_value : attribute -> passenger -> string
```

Les règles de conversion sont :
- `AgeGroup` : "child" si $\text{age} < 18$, "adult" si $18 \leq \text{age} \leq 60$, "senior" sinon
- `FamilySize` : "alone" si $\text{sibsp} + \text{parch} = 0$, "small" si $\leq 3$, "large" sinon
- `FareGroup` : "low" si $\text{fare} < 10$, "medium" si $10 \leq \text{fare} \leq 50$, "high" sinon



## Partie 3 : Entropie et gain d'information

L'algorithme ID3 utilise l'entropie pour mesurer l'entropie $S$ d'un ensemble de données, définie par :

$$
H(S) = -\sum_{c \in \{0, 1\}} p_c \log_2(p_c)
$$

où $p_c$ est la proportion d'éléments de classe $c$ dans $S$. Par convention, $0 \log_2(0) = 0$.

5. Écrire une fonction `entropy` qui calcule l'entropie d'une liste de passagers :

```ocaml
val entropy : passenger list -> float
```

*Indication :* On peut calculer $\log_2(x)$ avec `log x /. log 2.0`.

Le gain d'information d'un attribut $A$ par rapport à un ensemble $S$ mesure la réduction d'entropie obtenue en divisant $S$ selon les valeurs de $A$ :

$$
IG(S, A) = H(S) - \sum_{v \in \text{valeurs}(A)} \frac{|S_v|}{|S|} H(S_v)
$$

où $S_v = \{x \in S \mid A(x) = v\}$ est le sous-ensemble des éléments ayant la valeur $v$ pour l'attribut $A$.

6. Écrire une fonction `information_gain` qui calcule le gain d'information d'un attribut :

```ocaml
val information_gain : passenger list -> attribute -> float
```

7. Écrire une fonction `best_attribute` qui trouve l'attribut avec le meilleur gain d'information parmi une liste d'attributs :

```ocaml
val best_attribute : passenger list -> attribute list -> attribute option
(* Retourne None si aucun attribut n'a un gain > 0 *)
```

## Partie 4 : Construction de l'arbre

Un arbre de décision est soit une feuille (prédiction), soit un nœud interne (test sur un attribut).

8. Définir le type `decision_tree` :

```ocaml
type decision_tree =
  | Leaf of int                                    (* Prédiction : 0 ou 1 *)
  | Node of attribute * (string * decision_tree) list  (* Attribut et branches *)
```

9. Écrire une fonction `majority_class` qui retourne la classe majoritaire (0 pour décédé, 1 pour survivant) d'un ensemble de passagers :

```ocaml
val majority_class : passenger list -> int
```

10. Écrire la fonction récursive `build_tree` qui construit un arbre de décision avec l'algorithme ID3 :

```ocaml
val build_tree : passenger list -> attribute list -> decision_tree
(* build_tree data attributes *)
```

L'algorithme ID3 est le suivant :
- Si la liste est vide, retourner une feuille avec la prédiction 0
- Si tous les passagers ont la même classe, retourner une feuille avec cette classe
- Si la liste d'attributs est vide, retourner une feuille avec la classe majoritaire
- Sinon :
   - Trouver le meilleur attribut $A$ selon le gain d'information
   - Pour chaque valeur $v$ de $A$, construire récursivement un sous-arbre sur le sous-ensemble $\{x \mid A(x) = v\}$
   - Retourner un nœud avec l'attribut $A$ et les sous-arbres

Utiliser la fonction suivante pour afficher l'arbre obtenu :

```ocaml
let rec print_tree tree indent =
  let spaces = String.make (indent * 2) ' ' in
  match tree with
  | Leaf pred -> 
    Printf.printf "%s└── Prédiction: %s\n" spaces 
      (if pred = 1 then "Survit" else "Décède")
  | Node (attr, branches) ->
    Printf.printf "%s[%s]\n" spaces (attribute_name attr);
    List.iter (fun (value, subtree) ->
      Printf.printf "%s  ├─ %s :\n" spaces value;
      print_tree subtree (indent + 2)
    ) branches
```

## Partie 5 : Prédiction et évaluation

11. Écrire une fonction `predict` qui prédit la survie d'un passager avec l'arbre de décision :

```ocaml
val predict : decision_tree -> passenger -> int
```

12. Écrire une fonction `accuracy` qui calcule la précision de l'arbre sur un ensemble de données :

```ocaml
val accuracy : decision_tree -> passenger list -> float
```

13. Tester l'arbre de décision avec le code suivant :

```ocaml
let () =
  let train = load_train () in
  let all_attrs = [Sex; Pclass; AgeGroup; FamilySize; Embarked; FareGroup] in
  let tree = build_tree train all_attrs in
  print_tree tree 0;
  Printf.printf "Précision: %.2f%%\n" (accuracy tree train *. 100.0)
```

## Partie 6 : Génération de soumission

14. Écrire une fonction `generate_submission` qui génère un fichier CSV de soumission au format Kaggle :

```ocaml
val generate_submission : decision_tree -> passenger list -> string -> unit
(* generate_submission tree test_data filename *)
```

Le fichier doit avoir le format suivant :
```
PassengerId,Survived
892,0
893,1
...
```

## Pour aller plus loin

15. Implémenter une fonction de validation croisée (5-fold) pour évaluer la performance réelle du modèle.

16. Implémenter l'algorithme C4.5, une amélioration d'ID3 qui utilise le ratio de gain au lieu du gain d'information pour éviter de favoriser les attributs avec beaucoup de valeurs :

$$
GR(S, A) = \frac{IG(S, A)}{IV(A)}
$$

où $IV(A) = -\sum_v \frac{|S_v|}{|S|} \log_2 \frac{|S_v|}{|S|}$ est la valeur intrinsèque de l'attribut $A$.

17. Implémenter une forêt aléatoire en construisant plusieurs arbres sur des sous-ensembles aléatoires des données et des attributs.
